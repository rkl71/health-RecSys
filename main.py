import streamlit as st

from langchain.memory import ConversationBufferMemory
from utils import qa_agent

st.title("ğŸ’Š ê±´ê°• ë³´ì¡°í’ˆ ì¶”ì²œ ì‹œìŠ¤í…œ")

with st.sidebar:
    openai_api_key = st.text_input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”ï¼š", type="password")
    st.markdown("[OpenAI API key ê°€ì ¸ì˜¤ì„¸ìš”](https://platform.openai.com/account/api-keys)")

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”: ", type="pdf")
question = st.text_input("ì¦ìƒì„ ì…ë ¥í•˜ì„¸ìš”:", disabled=not uploaded_file)

if uploaded_file and question and not openai_api_key:
    st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if uploaded_file and question and openai_api_key:
    with st.spinner("AIê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
        response = qa_agent(openai_api_key, st.session_state["memory"], uploaded_file, question)

    if "error" in response:
        st.error(response["error"])
    else:
        st.write("### ëŒ€ë‹µ")
        st.write(response["answer"])
    st.session_state["chat_history"] = response["chat_history"]

if "chat_history" in st.session_state:
    with st.expander("ì—­ì‚¬ ì†Œì‹"):
        for i in range(0, len(st.session_state["chat_history"]), 2):
            human_message = st.session_state["chat_history"][i]
            ai_message = st.session_state["chat_history"][i + 1]
            st.write(human_message.content)
            st.write(ai_message.content)
            if i < len(st.session_state["chat_history"]) - 2:
                st.divider()
